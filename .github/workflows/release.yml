name: Release

on:
  push:
    tags:
      - "v*.*.*"

permissions:
  contents: write
  packages: write

jobs:
  # Build GPU-enabled whisper.cpp binaries
  # AI features only available on platforms with GPU:
  # - macOS ARM64: Metal GPU
  # - Windows AMD64: CUDA GPU (NVIDIA required)
  # Other platforms: AI features disabled (CPU too slow for good UX)
  build-whisper-darwin-arm64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: macos-14
    steps:
      - uses: actions/checkout@v5
      - name: Build whisper.cpp with Metal
        run: |
          git clone --depth 1 --branch v1.8.2 https://github.com/ggerganov/whisper.cpp
          cd whisper.cpp
          # Enable Metal GPU acceleration for Apple Silicon
          cmake -B build -DCMAKE_BUILD_TYPE=Release -DWHISPER_METAL=ON
          cmake --build build --config Release -j$(sysctl -n hw.ncpu)
          mkdir -p ../internal/core/ai/transcriber/bin
          cp build/bin/whisper-cli ../internal/core/ai/transcriber/bin/whisper-darwin-arm64
      - uses: actions/upload-artifact@v4
        with:
          name: whisper-darwin-arm64
          path: internal/core/ai/transcriber/bin/whisper-darwin-arm64

  build-whisper-windows-amd64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v5
      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.19
        with:
          cuda: '12.6.3'
          method: 'network'
      - name: Build whisper.cpp with CUDA
        shell: bash
        run: |
          git clone --depth 1 --branch v1.8.2 https://github.com/ggerganov/whisper.cpp
          cd whisper.cpp
          # Enable CUDA for NVIDIA GPU acceleration
          cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=ON
          cmake --build build --config Release -j
          mkdir -p ../internal/core/ai/transcriber/bin
          cp build/bin/Release/whisper-cli.exe ../internal/core/ai/transcriber/bin/whisper-windows-amd64.exe
      - uses: actions/upload-artifact@v4
        with:
          name: whisper-windows-amd64
          path: internal/core/ai/transcriber/bin/whisper-windows-amd64.exe

  # Build sherpa-onnx for Parakeet models
  # macOS ARM64: CPU (onnxruntime has CoreML via ANE)
  # Windows AMD64: CUDA GPU acceleration
  build-sherpa-darwin-arm64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: macos-14
    steps:
      - uses: actions/checkout@v5
      - name: Build sherpa-onnx
        run: |
          git clone --depth 1 --branch v1.12.20 https://github.com/k2-fsa/sherpa-onnx
          cd sherpa-onnx
          cmake -B build -DCMAKE_BUILD_TYPE=Release \
            -DSHERPA_ONNX_ENABLE_TTS=OFF \
            -DSHERPA_ONNX_ENABLE_SPEAKER_DIARIZATION=OFF \
            -DSHERPA_ONNX_ENABLE_BINARY=ON \
            -DBUILD_SHARED_LIBS=OFF \
            -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
            -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF
          cmake --build build --config Release -j$(sysctl -n hw.ncpu)
          mkdir -p ../internal/core/ai/transcriber/bin
          cp build/bin/sherpa-onnx-offline ../internal/core/ai/transcriber/bin/sherpa-darwin-arm64
      - uses: actions/upload-artifact@v4
        with:
          name: sherpa-darwin-arm64
          path: internal/core/ai/transcriber/bin/sherpa-darwin-arm64

  build-sherpa-windows-amd64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: windows-latest
    steps:
      - uses: actions/checkout@v5
      - name: Install CUDA Toolkit
        uses: Jimver/cuda-toolkit@v0.2.19
        with:
          cuda: '12.6.3'
          method: 'network'
      - name: Build sherpa-onnx with CUDA
        shell: bash
        run: |
          git clone --depth 1 --branch v1.12.20 https://github.com/k2-fsa/sherpa-onnx
          cd sherpa-onnx
          # Download onnxruntime GPU for CUDA 12.x
          curl -SL -O https://github.com/microsoft/onnxruntime/releases/download/v1.22.0/onnxruntime-win-x64-gpu-1.22.0.zip
          unzip onnxruntime-win-x64-gpu-1.22.0.zip
          export SHERPA_ONNXRUNTIME_LIB_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/lib
          export SHERPA_ONNXRUNTIME_INCLUDE_DIR=$PWD/onnxruntime-win-x64-gpu-1.22.0/include
          cmake -B build -DCMAKE_BUILD_TYPE=Release \
            -DSHERPA_ONNX_ENABLE_GPU=ON \
            -DSHERPA_ONNX_ENABLE_TTS=OFF \
            -DSHERPA_ONNX_ENABLE_SPEAKER_DIARIZATION=OFF \
            -DSHERPA_ONNX_ENABLE_BINARY=ON \
            -DBUILD_SHARED_LIBS=ON \
            -DSHERPA_ONNX_ENABLE_PORTAUDIO=OFF \
            -DSHERPA_ONNX_ENABLE_WEBSOCKET=OFF
          cmake --build build --config Release -j
          mkdir -p ../internal/core/ai/transcriber/bin
          cp build/bin/Release/sherpa-onnx-offline.exe ../internal/core/ai/transcriber/bin/sherpa-windows-amd64.exe
          # Copy onnxruntime DLLs for CUDA
          cp onnxruntime-win-x64-gpu-1.22.0/lib/*.dll ../internal/core/ai/transcriber/bin/
      - uses: actions/upload-artifact@v4
        with:
          name: sherpa-windows-amd64
          path: internal/core/ai/transcriber/bin/

  release:
    needs:
      - build-whisper-darwin-arm64
      - build-whisper-windows-amd64
      - build-sherpa-darwin-arm64
      - build-sherpa-windows-amd64
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Download AI binaries (whisper + sherpa-onnx)
        uses: actions/download-artifact@v4
        with:
          path: internal/core/ai/transcriber/bin/
          merge-multiple: true

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.25.4"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"

      - name: Build UI
        run: |
          cd ui && npm install && npm run build
          cd ..
          mkdir -p internal/server/dist
          rm -rf internal/server/dist/*
          cp -r ui/dist/* internal/server/dist/

      - name: Run GoReleaser
        uses: goreleaser/goreleaser-action@v6
        with:
          distribution: goreleaser
          version: "~> v2"
          args: release --clean
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          TAP_GITHUB_TOKEN: ${{ secrets.TAP_GITHUB_TOKEN }}

      - name: Extract version
        id: version
        run: echo "VERSION=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT

      - name: Prepare latest zips
        run: |
          mkdir -p latest
          cp dist/vget_${{ steps.version.outputs.VERSION }}_darwin_amd64.zip latest/vget-darwin-amd64.zip
          cp dist/vget_${{ steps.version.outputs.VERSION }}_darwin_arm64.zip latest/vget-darwin-arm64.zip
          cp dist/vget_${{ steps.version.outputs.VERSION }}_linux_amd64.zip latest/vget-linux-amd64.zip
          cp dist/vget_${{ steps.version.outputs.VERSION }}_linux_arm64.zip latest/vget-linux-arm64.zip
          cp dist/vget_${{ steps.version.outputs.VERSION }}_windows_amd64.zip latest/vget-windows-amd64.zip

      - name: Upload latest zips
        uses: softprops/action-gh-release@v2
        with:
          files: |
            latest/vget-darwin-amd64.zip
            latest/vget-darwin-arm64.zip
            latest/vget-linux-amd64.zip
            latest/vget-linux-arm64.zip
            latest/vget-windows-amd64.zip
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Build single Docker image with whisper.cpp and sherpa-onnx
  # No models bundled - downloads on demand or uses cloud API
  #
  # Runtime behavior:
  #   - GPU detected (nvidia-smi) → Local mode, download models from HuggingFace/vmirror
  #   - No GPU → API mode (OpenAI Whisper API, Groq, etc.)
  #
  # Image tags:
  #   :latest - multi-arch (amd64/arm64), auto-detects GPU at runtime

  docker-amd64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: ubuntu-latest
    outputs:
      digest: ${{ steps.build.outputs.digest }}
    steps:
      - uses: actions/checkout@v5
      - uses: docker/setup-buildx-action@v3
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build and push
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/vget/Dockerfile
          platforms: linux/amd64
          build-args: ENABLE_CUDA=true
          outputs: type=image,name=ghcr.io/${{ github.repository }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=gha,scope=amd64
          cache-to: type=gha,scope=amd64,mode=max

  docker-arm64:
    if: ${{ !contains(github.ref_name, '-') }}
    runs-on: ubuntu-24.04-arm
    outputs:
      digest: ${{ steps.build.outputs.digest }}
    steps:
      - uses: actions/checkout@v5
      - uses: docker/setup-buildx-action@v3
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Build and push
        id: build
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/vget/Dockerfile
          platforms: linux/arm64
          outputs: type=image,name=ghcr.io/${{ github.repository }},push-by-digest=true,name-canonical=true,push=true
          cache-from: type=gha,scope=arm64
          cache-to: type=gha,scope=arm64,mode=max

  # Create multi-arch manifest
  docker-manifest:
    runs-on: ubuntu-latest
    needs:
      - docker-amd64
      - docker-arm64
    steps:
      - uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract version
        id: version
        run: echo "VERSION=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT

      - name: Create manifest
        run: |
          IMAGE="ghcr.io/${{ github.repository }}"
          VERSION="${{ steps.version.outputs.VERSION }}"

          # :latest (multi-arch, no models bundled, runtime GPU detection)
          docker buildx imagetools create -t "$IMAGE:latest" -t "$IMAGE:$VERSION" \
            "$IMAGE@${{ needs.docker-amd64.outputs.digest }}" \
            "$IMAGE@${{ needs.docker-arm64.outputs.digest }}"

          echo "✅ Published Docker image (multi-arch amd64/arm64):"
          echo "  - $IMAGE:latest"
          echo "  - $IMAGE:$VERSION"
          echo ""
          echo "Runtime behavior:"
          echo "  - With NVIDIA GPU (--gpus all): Local transcription, download models on demand"
          echo "  - Without GPU: Use cloud API (OpenAI Whisper, Groq, etc.)"
